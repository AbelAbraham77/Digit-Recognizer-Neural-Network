{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### DIGIT RECOGNISER NEURAL NETWORK\n",
    "(using only NumPy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this notebook, I aim to implement a basic two-layer neural network and trained it on the MNIST digit recogniser dataset which contains sample hand-written digits of 28 x 28 pixel resolution. This was largely done with the help of Samson Zhang's YouTube tutorial."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np \n",
    "import pandas as pd \n",
    "from matplotlib import pyplot as plt \n",
    "\n",
    "data = pd.read_csv('train.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = np.array(data)\n",
    "m, n = data.shape\n",
    "\n",
    "#shuffles the databefore splitting it into dev and training sets\n",
    "np.random.shuffle(data)\n",
    "\n",
    "data_dev = data[0:1000].T \n",
    "Y_dev = data_dev[0]\n",
    "X_dev = data_dev[1:n]\n",
    "X_dev = X_dev / 255.\n",
    "\n",
    "data_train = data[1000:m].T \n",
    "Y_train = data_train[0]\n",
    "X_train = data_train[1:n]\n",
    "X_train = X_train / 255.\n",
    "_, m_train = X_train.shape\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([7, 0, 8, ..., 3, 9, 1], dtype=int64)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Y_train"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The Neural Network will have a two-layer architecture. The input layer $a^{[0]}$ will have 784 units corresponding to the pixels in the input image (28 x 28 pixels). The hidden layer $a^{[1]}$ will have 10 units with ReLU activation, and the output layer $a^{[2]}$ will have 10 units corresponding to the ten digit classes we aim to classify the image into, with softmax activation.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The Math\n",
    "\n",
    "**Forward Propagation**\n",
    "\n",
    "$$Z^{[1]} = W^{[1]}X + b^{[1]}$$\n",
    "$$A^{[1]} = g_{\\text{ReLU}}(Z^{[1]})$$\n",
    "$$Z^{[2]} = W^{[2]}A^{[1]} + b^{[2]}$$\n",
    "$$A^{[2]} = g_{\\text{softmax}}(Z^{[2]})$$\n",
    "\n",
    "**Backward propagation**\n",
    "\n",
    "$$dZ^{[2]} = A^{[2]} - Y$$\n",
    "$$dW^{[2]} = \\frac{1}{m} dZ^{[2]} A^{[1]T}$$\n",
    "$$dB^{[2]} = \\frac{1}{m} \\Sigma {dZ^{[2]}}$$\n",
    "$$dZ^{[1]} = W^{[2]T} dZ^{[2]} .* g^{[1]\\prime} (z^{[1]})$$\n",
    "$$dW^{[1]} = \\frac{1}{m} dZ^{[1]} A^{[0]T}$$\n",
    "$$dB^{[1]} = \\frac{1}{m} \\Sigma {dZ^{[1]}}$$\n",
    "\n",
    "**Parameter updates**\n",
    "\n",
    "$$W^{[2]} := W^{[2]} - \\alpha dW^{[2]}$$\n",
    "$$b^{[2]} := b^{[2]} - \\alpha db^{[2]}$$\n",
    "$$W^{[1]} := W^{[1]} - \\alpha dW^{[1]}$$\n",
    "$$b^{[1]} := b^{[1]} - \\alpha db^{[1]}$$\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def init_parameters():\n",
    "    W1 = np.random.rand(10, 784) - 0.5\n",
    "    b1 = np.random.rand(10, 1) - 0.5\n",
    "    W2 = np.random.rand(10, 10) - 0.5\n",
    "    b2 = np.random.rand(10, 1) - 0.5\n",
    "    return W1, b1, W2, b2\n",
    "\n",
    "def ReLU(Z):\n",
    "    return np.maximum(Z, 0)\n",
    "\n",
    "def SoftMax(Z):\n",
    "    A = np.exp(Z) / sum(np.exp(Z))\n",
    "    return A\n",
    "    \n",
    "def forward_prop(W1, b1, W2, b2, X):\n",
    "    Z1 = W1.dot(X) + b1\n",
    "    A1 = ReLU(Z1)\n",
    "    Z2 = W2.dot(A1) + b2\n",
    "    A2 = SoftMax(Z2)\n",
    "    return Z1, A1, Z2, A2\n",
    "\n",
    "def ReLU_deriv(Z):\n",
    "    return Z > 0\n",
    "\n",
    "def one_hot(Y):\n",
    "    one_hot_Y = np.zeros((Y.size, Y.max() + 1))\n",
    "    one_hot_Y[np.arange(Y.size), Y] = 1\n",
    "    one_hot_Y = one_hot_Y.T\n",
    "    return one_hot_Y\n",
    "\n",
    "def backward_prop(Z1, A1, Z2, A2, W1, W2, X, Y):\n",
    "    one_hot_Y = one_hot(Y)\n",
    "    dZ2 = A2 - one_hot_Y\n",
    "    dW2 = 1 / m * dZ2.dot(A1.T)\n",
    "    db2 = 1 / m * np.sum(dZ2)\n",
    "    dZ1 = W2.T.dot(dZ2) * ReLU_deriv(Z1)\n",
    "    dW1 = 1 / m * dZ1.dot(X.T)\n",
    "    db1 = 1 / m * np.sum(dZ1)\n",
    "    return dW1, db1, dW2, db2\n",
    "\n",
    "def update_parameters(W1, b1, W2, b2, dW1, db1, dW2, db2, alpha):\n",
    "    W1 = W1 - alpha * dW1\n",
    "    b1 = b1 - alpha * db1    \n",
    "    W2 = W2 - alpha * dW2  \n",
    "    b2 = b2 - alpha * db2    \n",
    "    return W1, b1, W2, b2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_predictions(A2):\n",
    "    return np.argmax(A2, 0)\n",
    "\n",
    "def get_accuracy(predictions, Y):\n",
    "    print(predictions, Y)\n",
    "    return np.sum(predictions == Y) / Y.size\n",
    "\n",
    "def gradient_descent(X, Y, alpha, iterations):\n",
    "    W1, b1, W2, b2 = init_parameters()\n",
    "    for i in range(iterations):\n",
    "        Z1, A1, Z2, A2 = forward_prop(W1, b1, W2, b2, X)\n",
    "        dW1, db1, dW2, db2 = backward_prop(Z1, A1, Z2, A2, W1, W2, X, Y)\n",
    "        W1, b1, W2, b2 = update_parameters(W1, b1, W2, b2, dW1, db1, dW2, db2, alpha)\n",
    "        if i % 50 == 0:\n",
    "            print(\"Iteration: \", i)\n",
    "            predictions = get_predictions(A2)\n",
    "            print(get_accuracy(predictions, Y))\n",
    "    return W1, b1, W2, b2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration:  0\n",
      "[6 6 1 ... 2 3 2] [7 0 8 ... 3 9 1]\n",
      "0.08790243902439024\n",
      "Iteration:  10\n",
      "[6 6 3 ... 2 3 2] [7 0 8 ... 3 9 1]\n",
      "0.13609756097560977\n",
      "Iteration:  20\n",
      "[6 6 1 ... 2 3 2] [7 0 8 ... 3 9 1]\n",
      "0.2068780487804878\n",
      "Iteration:  30\n",
      "[6 6 1 ... 2 3 1] [7 0 8 ... 3 9 1]\n",
      "0.2578780487804878\n",
      "Iteration:  40\n",
      "[6 6 1 ... 8 3 1] [7 0 8 ... 3 9 1]\n",
      "0.29236585365853657\n",
      "Iteration:  50\n",
      "[6 0 2 ... 8 3 1] [7 0 8 ... 3 9 1]\n",
      "0.3238780487804878\n",
      "Iteration:  60\n",
      "[6 0 2 ... 8 3 1] [7 0 8 ... 3 9 1]\n",
      "0.3550243902439024\n",
      "Iteration:  70\n",
      "[6 0 8 ... 8 3 1] [7 0 8 ... 3 9 1]\n",
      "0.3893658536585366\n",
      "Iteration:  80\n",
      "[0 0 8 ... 6 6 1] [7 0 8 ... 3 9 1]\n",
      "0.4200243902439024\n",
      "Iteration:  90\n",
      "[0 0 8 ... 6 6 1] [7 0 8 ... 3 9 1]\n",
      "0.45021951219512196\n",
      "Iteration:  100\n",
      "[0 0 8 ... 6 6 1] [7 0 8 ... 3 9 1]\n",
      "0.4804390243902439\n",
      "Iteration:  110\n",
      "[0 0 8 ... 6 6 1] [7 0 8 ... 3 9 1]\n",
      "0.5123414634146342\n",
      "Iteration:  120\n",
      "[0 0 8 ... 6 6 1] [7 0 8 ... 3 9 1]\n",
      "0.5406829268292683\n",
      "Iteration:  130\n",
      "[0 0 8 ... 6 6 1] [7 0 8 ... 3 9 1]\n",
      "0.5626585365853658\n",
      "Iteration:  140\n",
      "[0 0 8 ... 6 6 1] [7 0 8 ... 3 9 1]\n",
      "0.5836585365853658\n",
      "Iteration:  150\n",
      "[6 0 8 ... 6 6 1] [7 0 8 ... 3 9 1]\n",
      "0.6012439024390244\n",
      "Iteration:  160\n",
      "[6 0 8 ... 6 9 1] [7 0 8 ... 3 9 1]\n",
      "0.6186341463414634\n",
      "Iteration:  170\n",
      "[4 0 8 ... 6 9 1] [7 0 8 ... 3 9 1]\n",
      "0.6329024390243903\n",
      "Iteration:  180\n",
      "[4 0 8 ... 6 9 1] [7 0 8 ... 3 9 1]\n",
      "0.6457560975609756\n",
      "Iteration:  190\n",
      "[4 0 8 ... 6 9 1] [7 0 8 ... 3 9 1]\n",
      "0.6582926829268293\n",
      "Iteration:  200\n",
      "[4 0 8 ... 6 9 1] [7 0 8 ... 3 9 1]\n",
      "0.6704390243902439\n",
      "Iteration:  210\n",
      "[4 0 8 ... 6 9 1] [7 0 8 ... 3 9 1]\n",
      "0.6820975609756098\n",
      "Iteration:  220\n",
      "[4 0 8 ... 6 9 1] [7 0 8 ... 3 9 1]\n",
      "0.6923170731707317\n",
      "Iteration:  230\n",
      "[4 0 8 ... 6 9 1] [7 0 8 ... 3 9 1]\n",
      "0.7001951219512195\n",
      "Iteration:  240\n",
      "[4 0 8 ... 6 9 1] [7 0 8 ... 3 9 1]\n",
      "0.7096097560975609\n",
      "Iteration:  250\n",
      "[4 0 8 ... 6 9 1] [7 0 8 ... 3 9 1]\n",
      "0.7177073170731707\n",
      "Iteration:  260\n",
      "[4 0 8 ... 6 9 1] [7 0 8 ... 3 9 1]\n",
      "0.7250487804878049\n",
      "Iteration:  270\n",
      "[4 0 8 ... 6 9 1] [7 0 8 ... 3 9 1]\n",
      "0.7319024390243902\n",
      "Iteration:  280\n",
      "[4 0 8 ... 6 9 1] [7 0 8 ... 3 9 1]\n",
      "0.7390243902439024\n",
      "Iteration:  290\n",
      "[4 0 8 ... 6 9 1] [7 0 8 ... 3 9 1]\n",
      "0.7453170731707317\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[13], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m W1, b1, W2, b2 \u001b[38;5;241m=\u001b[39m \u001b[43mgradient_descent\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX_train\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mY_train\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m0.10\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m500\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[1;32mIn[12], line 11\u001b[0m, in \u001b[0;36mgradient_descent\u001b[1;34m(X, Y, alpha, iterations)\u001b[0m\n\u001b[0;32m      9\u001b[0m W1, b1, W2, b2 \u001b[38;5;241m=\u001b[39m init_parameters()\n\u001b[0;32m     10\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(iterations):\n\u001b[1;32m---> 11\u001b[0m     Z1, A1, Z2, A2 \u001b[38;5;241m=\u001b[39m \u001b[43mforward_prop\u001b[49m\u001b[43m(\u001b[49m\u001b[43mW1\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mb1\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mW2\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mb2\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mX\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     12\u001b[0m     dW1, db1, dW2, db2 \u001b[38;5;241m=\u001b[39m backward_prop(Z1, A1, Z2, A2, W1, W2, X, Y)\n\u001b[0;32m     13\u001b[0m     W1, b1, W2, b2 \u001b[38;5;241m=\u001b[39m update_parameters(W1, b1, W2, b2, dW1, db1, dW2, db2, alpha)\n",
      "Cell \u001b[1;32mIn[11], line 16\u001b[0m, in \u001b[0;36mforward_prop\u001b[1;34m(W1, b1, W2, b2, X)\u001b[0m\n\u001b[0;32m     15\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward_prop\u001b[39m(W1, b1, W2, b2, X):\n\u001b[1;32m---> 16\u001b[0m     Z1 \u001b[38;5;241m=\u001b[39m \u001b[43mW1\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdot\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;241m+\u001b[39m b1\n\u001b[0;32m     17\u001b[0m     A1 \u001b[38;5;241m=\u001b[39m ReLU(Z1)\n\u001b[0;32m     18\u001b[0m     Z2 \u001b[38;5;241m=\u001b[39m W2\u001b[38;5;241m.\u001b[39mdot(A1) \u001b[38;5;241m+\u001b[39m b2\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "W1, b1, W2, b2 = gradient_descent(X_train, Y_train, 0.10, 500)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
